# library_loader_enhanced.py
import json
import os
import re
from typing import List, Dict, Any, Optional
from pathlib import Path

class WorkflowsLibraryLoader:
    """Ù…Ø­Ù…Ù„ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù€ workflows Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø¨Ø­Ø« ÙˆÙÙ‡Ø±Ø³Ø© Ø°ÙƒÙŠ"""
    
    def __init__(self):
        self.workflows = []
        self.indexed_data = {
            'keywords': {},
            'services': {},
            'triggers': {},
            'patterns': {}
        }
        self.load_workflows_from_files()
    
    def load_workflows_from_files(self):
        """ØªØ­Ù…ÙŠÙ„ workflows Ù…Ù† Ù…Ø¬Ù„Ø¯ workflows/"""
        workflows_dir = Path("workflows")
        
        if not workflows_dir.exists():
            print("[WARNING] Workflows directory not found")
            return
        
        loaded_count = 0
        for workflow_file in workflows_dir.glob("*.json"):
            try:
                with open(workflow_file, 'r', encoding='utf-8') as f:
                    workflow_data = json.load(f)
                    processed_workflow = self.process_workflow(workflow_data, workflow_file.name)
                    if processed_workflow:
                        self.workflows.append(processed_workflow)
                        loaded_count += 1
            except Exception as e:
                print(f"[ERROR] Failed to load {workflow_file}: {e}")
        
        print(f"[SUCCESS] Loaded {loaded_count} workflows from library")
        self.build_index()
    
    def process_workflow(self, raw_workflow: Dict, filename: str) -> Optional[Dict]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© workflow ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙŠØ§ØªÙ‡"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            name = raw_workflow.get('name', filename.replace('.json', ''))
            nodes = raw_workflow.get('nodes', [])
            
            if not nodes:
                return None
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø¯ ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª
            services = self.extract_services(nodes)
            trigger_types = self.extract_triggers(nodes)
            patterns = self.extract_patterns(name, nodes)
            complexity = self.calculate_complexity(nodes)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„ÙˆØµÙ
            description = self.generate_description(name, services, trigger_types)
            keywords = self.extract_keywords(name, description, services)
            
            return {
                'filename': filename,
                'name': name,
                'description': description,
                'raw_workflow': raw_workflow,
                'services': services,
                'trigger_types': trigger_types,
                'patterns': patterns,
                'keywords': keywords,
                'complexity': complexity,
                'node_count': len(nodes),
                'active': raw_workflow.get('active', True)
            }
        except Exception as e:
            print(f"[ERROR] Failed to process workflow {filename}: {e}")
            return None
    
    def extract_services(self, nodes: List[Dict]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯"""
        services = set()
        service_mapping = {
            'googleSheets': 'google-sheets',
            'gmail': 'gmail',
            'slack': 'slack',
            'webhook': 'webhook',
            'httpRequest': 'http',
            'telegram': 'telegram',
            'discord': 'discord',
            'openai': 'openai',
            'airtable': 'airtable',
            'notion': 'notion',
            'wordpress': 'wordpress',
            'shopify': 'shopify',
            'hubspot': 'hubspot',
            'salesforce': 'salesforce',
            'mailchimp': 'mailchimp',
            'twilio': 'twilio'
        }
        
        for node in nodes:
            node_type = node.get('type', '')
            for service_key, service_name in service_mapping.items():
                if service_key.lower() in node_type.lower():
                    services.add(service_name)
                    break
        
        return list(services)
    
    def extract_triggers(self, nodes: List[Dict]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø´ØºÙ„Ø§Øª"""
        triggers = set()
        trigger_types = {
            'webhook': 'webhook',
            'cron': 'schedule',
            'manualTrigger': 'manual',
            'emailTrigger': 'email',
            'telegram': 'telegram',
            'discord': 'discord'
        }
        
        for node in nodes:
            node_type = node.get('type', '').lower()
            for trigger_key, trigger_name in trigger_types.items():
                if trigger_key.lower() in node_type:
                    triggers.add(trigger_name)
        
        return list(triggers) if triggers else ['manual']
    
    def extract_patterns(self, name: str, nodes: List[Dict]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        patterns = []
        name_lower = name.lower()
        
        # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø´Ø§Ø¦Ø¹Ø©
        if any(word in name_lower for word in ['form', 'contact', 'submission']):
            patterns.append('form_processing')
        
        if any(word in name_lower for word in ['email', 'mail', 'notification']):
            patterns.append('email_automation')
        
        if any(word in name_lower for word in ['schedule', 'daily', 'cron']):
            patterns.append('scheduled_task')
        
        if any(word in name_lower for word in ['chat', 'bot', 'assistant']):
            patterns.append('chatbot')
        
        if any(word in name_lower for word in ['sync', 'integration', 'connect']):
            patterns.append('data_sync')
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø¯
        node_types = [node.get('type', '').lower() for node in nodes]
        
        if any('sheets' in nt for nt in node_types):
            patterns.append('spreadsheet_integration')
        
        if any('ai' in nt or 'openai' in nt for nt in node_types):
            patterns.append('ai_powered')
        
        return patterns
    
    def calculate_complexity(self, nodes: List[Dict]) -> str:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù€ workflow"""
        node_count = len(nodes)
        
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity_score = node_count
        
        # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· Ù„Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
        for node in nodes:
            node_type = node.get('type', '').lower()
            if any(complex_type in node_type for complex_type in ['function', 'code', 'if', 'switch', 'merge']):
                complexity_score += 2
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        if complexity_score <= 5:
            return 'low'
        elif complexity_score <= 15:
            return 'medium'
        else:
            return 'high'
    
    def generate_description(self, name: str, services: List[str], triggers: List[str]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù€ workflow"""
        trigger_desc = triggers[0] if triggers else 'manual'
        services_desc = ', '.join(services[:3]) if services else 'basic operations'
        
        return f"Automated workflow using {trigger_desc} trigger with {services_desc} integration"
    
    def extract_keywords(self, name: str, description: str, services: List[str]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„ÙÙ‡Ø±Ø³Ø©"""
        keywords = set()
        
        # Ù…Ù† Ø§Ù„Ø§Ø³Ù…
        name_words = re.findall(r'\w+', name.lower())
        keywords.update(name_words)
        
        # Ù…Ù† Ø§Ù„ÙˆØµÙ
        desc_words = re.findall(r'\w+', description.lower())
        keywords.update(desc_words)
        
        # Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª
        keywords.update(services)
        
        # Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø©
        stop_words = {'and', 'or', 'the', 'a', 'an', 'to', 'for', 'with', 'by'}
        keywords = keywords - stop_words
        
        return list(keywords)
    
    def build_index(self):
        """Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«"""
        for i, workflow in enumerate(self.workflows):
            # ÙÙ‡Ø±Ø³Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            for keyword in workflow['keywords']:
                if keyword not in self.indexed_data['keywords']:
                    self.indexed_data['keywords'][keyword] = []
                self.indexed_data['keywords'][keyword].append(i)
            
            # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª
            for service in workflow['services']:
                if service not in self.indexed_data['services']:
                    self.indexed_data['services'][service] = []
                self.indexed_data['services'][service].append(i)
            
            # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ù…Ø´ØºÙ„Ø§Øª
            for trigger in workflow['trigger_types']:
                if trigger not in self.indexed_data['triggers']:
                    self.indexed_data['triggers'][trigger] = []
                self.indexed_data['triggers'][trigger].append(i)
            
            # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            for pattern in workflow['patterns']:
                if pattern not in self.indexed_data['patterns']:
                    self.indexed_data['patterns'][pattern] = []
                self.indexed_data['patterns'][pattern].append(i)
    
    def search_workflows(self, query: str, services: List[str] = None, 
                        trigger_type: str = None, max_results: int = 10) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù€ workflows"""
        if not query.strip() and not services and not trigger_type:
            return self.workflows[:max_results]
        
        scores = {}
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ
        if query.strip():
            query_words = re.findall(r'\w+', query.lower())
            for word in query_words:
                # Ø¨Ø­Ø« Ø¯Ù‚ÙŠÙ‚
                if word in self.indexed_data['keywords']:
                    for idx in self.indexed_data['keywords'][word]:
                        scores[idx] = scores.get(idx, 0) + 3
                
                # Ø¨Ø­Ø« Ø¬Ø²Ø¦ÙŠ
                for keyword, indices in self.indexed_data['keywords'].items():
                    if word in keyword or keyword in word:
                        for idx in indices:
                            scores[idx] = scores.get(idx, 0) + 1
        
        # ÙÙ„ØªØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø§Øª
        if services:
            service_indices = set()
            for service in services:
                if service in self.indexed_data['services']:
                    service_indices.update(self.indexed_data['services'][service])
            
            if service_indices:
                for idx in service_indices:
                    scores[idx] = scores.get(idx, 0) + 5
        
        # ÙÙ„ØªØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØºÙ„
        if trigger_type and trigger_type in self.indexed_data['triggers']:
            for idx in self.indexed_data['triggers'][trigger_type]:
                scores[idx] = scores.get(idx, 0) + 3
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if not scores:
            return self.workflows[:max_results]
        
        sorted_results = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        results = []
        for idx, score in sorted_results[:max_results]:
            workflow = self.workflows[idx].copy()
            workflow['relevance_score'] = score
            results.append(workflow)
        
        return results
    
    def get_best_template_for_analysis(self, analysis_data: Dict) -> Optional[Dict]:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù‚Ø§Ù„Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        services = analysis_data.get('services', [])
        trigger_type = analysis_data.get('trigger_type', '')
        operations = analysis_data.get('operations', [])
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«
        search_query = ' '.join(operations + services)
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† templates Ù…Ù†Ø§Ø³Ø¨Ø©
        candidates = self.search_workflows(
            query=search_query,
            services=services,
            trigger_type=trigger_type,
            max_results=5
        )
        
        if candidates:
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ·Ø§Ø¨Ù‚
            best_match = candidates[0]
            return best_match
        
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ¨Ø©"""
        total_workflows = len(self.workflows)
        active_workflows = sum(1 for w in self.workflows if w['active'])
        unique_services = len(self.indexed_data['services'])
        
        complexity_dist = {}
        for workflow in self.workflows:
            complexity = workflow['complexity']
            complexity_dist[complexity] = complexity_dist.get(complexity, 0) + 1
        
        return {
            'total_workflows': total_workflows,
            'active_workflows': active_workflows,
            'unique_services': unique_services,
            'complexity_distribution': complexity_dist,
            'available_services': list(self.indexed_data['services'].keys()),
            'available_triggers': list(self.indexed_data['triggers'].keys()),
            'available_patterns': list(self.indexed_data['patterns'].keys())
        }

# ØªØ­Ø¯ÙŠØ« ai.py Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯
class EnhancedAISystem:
    """Ù†Ø¸Ø§Ù… AI Ù…Ø­Ø³Ù† Ù…Ø¹ Ù…ÙƒØªØ¨Ø© workflows"""
    
    def __init__(self):
        self.library_loader = WorkflowsLibraryLoader()
        
    async def plan_workflow_with_library(self, user_prompt: str) -> Tuple[str, bool]:
        """ØªØ®Ø·ÙŠØ· workflow Ù…Ø¹ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø©"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨
            analysis = await self.analyze_request_with_ai(user_prompt)
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø©
            relevant_workflows = self.library_loader.search_workflows(
                query=user_prompt,
                services=analysis.get('services', []),
                trigger_type=analysis.get('trigger_type'),
                max_results=5
            )
            
            # ØªØ­Ø¶ÙŠØ± ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„
            plan = self.create_comprehensive_plan(user_prompt, analysis, relevant_workflows)
            
            return plan, True
            
        except Exception as e:
            print(f"[ERROR] Enhanced planning failed: {e}")
            return f"ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ: {user_prompt}", False
    
    async def analyze_request_with_ai(self, user_prompt: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI"""
        analysis_prompt = f"""
Ø­Ù„Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ø¹Ù…Ù‚:
"{user_prompt}"

Ø§Ø³ØªØ®Ø±Ø¬:
1. Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØºÙ„ (webhook/schedule/manual)
2. Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
3. Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
4. Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø®ØµØµØ©
5. Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¹Ù…Ù„

Ø£Ø¬Ø¨ Ø¨Ù€ JSON ØµØ§Ù„Ø­:
{{
  "trigger_type": "...",
  "services": ["..."],
  "operations": ["..."],
  "custom_names": {{}},
  "business_logic": ["..."],
  "data_fields": {{}}
}}
"""
        
        try:
            response = await _call_gemini_api(analysis_prompt, ADVANCED_ANALYZER_PROMPT)
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            print(f"[WARNING] AI analysis failed: {e}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ
        return self.fallback_analysis(user_prompt)
    
    def fallback_analysis(self, user_prompt: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¨Ø³ÙŠØ·"""
        text = user_prompt.lower()
        
        # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØºÙ„
        if any(word in text for word in ['form', 'submit', 'webhook']):
            trigger = 'webhook'
        elif any(word in text for word in ['schedule', 'daily', 'every']):
            trigger = 'schedule'
        else:
            trigger = 'manual'
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø¯Ù…Ø§Øª
        services = []
        if 'sheets' in text or 'Ø¬Ø¯ÙˆÙ„' in text:
            services.append('google-sheets')
        if 'email' in text or 'gmail' in text:
            services.append('gmail')
        if 'slack' in text:
            services.append('slack')
        
        return {
            'trigger_type': trigger,
            'services': services,
            'operations': ['data_processing'],
            'custom_names': {},
            'business_logic': [],
            'data_fields': {}
        }
    
    def create_comprehensive_plan(self, user_prompt: str, analysis: Dict, 
                                 relevant_workflows: List[Dict]) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø·Ø© Ø´Ø§Ù…Ù„Ø©"""
        plan_parts = [
            "ğŸ” **ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø·Ù„Ø¨:**",
            "",
            f"**Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ØºÙ„:** {analysis.get('trigger_type', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}",
            f"**Ø§Ù„Ø®Ø¯Ù…Ø§Øª:** {', '.join(analysis.get('services', ['Ø£Ø³Ø§Ø³ÙŠØ©']))}",
            f"**Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª:** {', '.join(analysis.get('operations', ['Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙŠØ§Ù†Ø§Øª']))}",
        ]
        
        if analysis.get('custom_names'):
            plan_parts.extend([
                "",
                "**Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø®ØµØµØ©:**",
                json.dumps(analysis['custom_names'], ensure_ascii=False, indent=2)
            ])
        
        if relevant_workflows:
            plan_parts.extend([
                "",
                f"**workflows Ù…Ø´Ø§Ø¨Ù‡Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© ({len(relevant_workflows)}):**"
            ])
            
            for i, workflow in enumerate(relevant_workflows[:3], 1):
                plan_parts.append(f"{i}. {workflow['name']} (Ù…Ø·Ø§Ø¨Ù‚Ø©: {workflow.get('relevance_score', 0)})")
        
        plan_parts.extend([
            "",
            "**Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ØµÙ„ÙŠ:**",
            user_prompt
        ])
        
        return "\n".join(plan_parts)
    
    async def generate_custom_workflow(self, plan: str) -> Tuple[str, bool]:
        """ØªÙˆÙ„ÙŠØ¯ workflow Ù…Ø®ØµØµ Ù…Ø­Ø³Ù†"""
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø®Ø·Ø©
            user_request = plan.split("Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ØµÙ„ÙŠ:")[-1].strip()
            analysis = await self.analyze_request_with_ai(user_request)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ù‚Ø§Ù„Ø¨
            best_template = self.library_loader.get_best_template_for_analysis(analysis)
            
            if best_template:
                # ØªØ®ØµÙŠØµ Ø§Ù„Ù‚Ø§Ù„Ø¨
                customized_workflow = self.customize_workflow_from_template(
                    best_template['raw_workflow'], 
                    analysis
                )
                print(f"[SUCCESS] Customized workflow from template: {best_template['name']}")
            else:
                # Ø¥Ù†Ø´Ø§Ø¡ workflow Ø¬Ø¯ÙŠØ¯
                customized_workflow = self.create_new_workflow(analysis)
                print("[SUCCESS] Created new custom workflow")
            
            return json.dumps(customized_workflow, ensure_ascii=False, indent=2), True
            
        except Exception as e:
            print(f"[ERROR] Custom workflow generation failed: {e}")
            # Ø¥Ø±Ø¬Ø§Ø¹ workflow Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            fallback = self.create_fallback_workflow()
            return json.dumps(fallback, ensure_ascii=False, indent=2), False
    
    def customize_workflow_from_template(self, template: Dict[str, Any], 
                                       analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ØªØ®ØµÙŠØµ workflow Ù…Ù† Ù‚Ø§Ù„Ø¨ Ù…ÙˆØ¬ÙˆØ¯"""
        customized = copy.deepcopy(template)
        
        # ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        customized['name'] = self.generate_custom_name(analysis)
        customized['updatedAt'] = datetime.now().isoformat()
        
        # ØªØ®ØµÙŠØµ Ø§Ù„Ø¹Ù‚Ø¯
        for node in customized.get('nodes', []):
            self.customize_node_parameters(node, analysis)
        
        # Ø¥Ø¶Ø§ÙØ© Ø¹Ù‚Ø¯ Ø¬Ø¯ÙŠØ¯Ø© Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
        self.add_missing_nodes(customized, analysis)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª
        self.update_workflow_connections(customized)
        
        return customized
    
    def customize_node_parameters(self, node: Dict[str, Any], analysis: Dict[str, Any]):
        """ØªØ®ØµÙŠØµ parameters Ø§Ù„Ø¹Ù‚Ø¯Ø©"""
        node_type = node.get('type', '').lower()
        
        if 'googlesheets' in node_type:
            self.customize_sheets_node(node, analysis)
        elif 'gmail' in node_type:
            self.customize_gmail_node(node, analysis)
        elif 'slack' in node_type:
            self.customize_slack_node(node, analysis)
    
    def customize_sheets_node(self, node: Dict[str, Any], analysis: Dict[str, Any]):
        """ØªØ®ØµÙŠØµ Ø¹Ù‚Ø¯Ø© Google Sheets"""
        params = node.setdefault('parameters', {})
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù…Ø§Ø¡ Ù…Ø®ØµØµØ©
        custom_names = analysis.get('custom_names', {})
        if 'sheet_name' in custom_names:
            params['sheetName'] = {
                "__rl": True,
                "value": custom_names['sheet_name'],
                "mode": "list"
            }
        
        # ØªØ®ØµÙŠØµ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        data_fields = analysis.get('data_fields', {})
        if data_fields:
            columns = {}
            for field, description in data_fields.items():
                column_name = description.title() if description else field.title()
                columns[column_name] = f"={{{{ $json.{field} }}}}"
            
            # Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ©
            if 'generate_id' in analysis.get('business_logic', []):
                columns['Request_ID'] = "={{ 'REQ-' + new Date().getTime().toString() }}"
            
            columns['Timestamp'] = "={{ new Date().toISOString() }}"
            
            params['columns'] = {
                "mappingMode": "defineBelow",
                "value": columns
            }
    
    def generate_custom_name(self, analysis: Dict[str, Any]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³Ù… Ù…Ø®ØµØµ Ù„Ù„Ù€ workflow"""
        trigger = analysis.get('trigger_type', 'automation')
        services = analysis.get('services', [])
        
        name_parts = []
        
        if trigger == 'webhook':
            name_parts.append('Form')
        elif trigger == 'schedule':
            name_parts.append('Scheduled')
        
        if 'google-sheets' in services:
            name_parts.append('to Sheets')
        if 'gmail' in services:
            name_parts.append('with Email')
        
        # Ø¥Ø¶Ø§ÙØ© ØªØ®ØµÙŠØµ
        custom_names = analysis.get('custom_names', {})
        if custom_names:
            first_custom = list(custom_names.values())[0]
            name_parts.append(f'({first_custom})')
        
        return ' '.join(name_parts) if name_parts else 'Custom Automation'

# Singleton instance
enhanced_ai_system = EnhancedAISystem()

# ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†
async def plan_workflow_with_ai(user_prompt: str) -> Tuple[str, bool]:
    """Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„ØªØ®Ø·ÙŠØ·"""
    return await enhanced_ai_system.plan_workflow_with_library(user_prompt)

async def draft_n8n_json_with_ai(plan: str) -> Tuple[str, bool]:
    """Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù† Ù„Ù„ØªÙˆÙ„ÙŠØ¯"""
    return await enhanced_ai_system.generate_custom_workflow(plan)

def search_library_candidates(query: str, top_k: int = 3) -> List[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
    return enhanced_ai_system.library_loader.search_workflows(query, max_results=top_k)

def get_library_stats() -> Dict[str, Any]:
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…ÙƒØªØ¨Ø©"""
    return enhanced_ai_system.library_loader.get_stats()
